{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data clenaing and normalization:\n",
    "First steps were to select the data that we wnat to work with. So the attributes were selected and putted in a specific dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT LIBRARIES NECESSARY\n",
    "from IPython.display import display_html\n",
    "from IPython.display import Image \n",
    "from IPython.core.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import datetime as dt\n",
    "import requests\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the CSV dataset provided by IBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv('Data-Collisions.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a scoop on the dataset. We need to see what kind of attributes we have to work with and the type of each of them. Decided to get a more detailed look at the number of accidents associated with speeding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data frame shape:\", df.shape, '\\n')\n",
    "\n",
    "print(\"Data types:\\n\\n\",df.dtypes, '\\n')\n",
    "\n",
    "print(\"Amounts:\\n\", df['SPEEDING'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After being able to see a small sample of the dataset, we can see that we need to work on it. Since it is not a balanced labeled dataset, we need to process it and normalize the dataset. First set, before anything else, is to choose the attributes that we want to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [\"SEVERITYCODE\", \"INCDTTM\", \"WEATHER\", \"ROADCOND\", \"LIGHTCOND\", \"VEHCOUNT\", \"PERSONCOUNT\", 'PEDCOUNT', 'PEDCYLCOUNT']\n",
    "df_ac = df[col]\n",
    "df_ac[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Null or NaN values and reseting the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the null values from and reset the index\n",
    "df_ac = df_ac.dropna()\n",
    "df_ac = df_ac.reset_index()\n",
    "df_ac.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since the data format was not even for every row, we are creating a column with a normalized data (month-year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change INCDTTM to datetime format for extraction of year and month for new columns. \n",
    "df_ac['INCDTTM_dt'] = pd.to_datetime(df_ac.INCDTTM)\n",
    "df_ac['INCDTTM_year'] = df_ac.INCDTTM_dt.dt.year\n",
    "df_ac['INCDTTM_month'] = df_ac.INCDTTM_dt.dt.month\n",
    "df_ac['INCDTTM_year_str'] = df_ac.INCDTTM_year.astype(str)\n",
    "df_ac['INCDTTM_month_str'] = df_ac.INCDTTM_month.astype(str)\n",
    "df_ac['INCDTTM_year_month'] = df_ac[['INCDTTM_month_str', 'INCDTTM_year_str']].apply(lambda x: '-'.join(x),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normailizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dict = {'Overcast':1, 'Raining':2, 'Clear':3, 'Other':4, 'Snowing':5,\n",
    "               'Fog/Smog/Smoke':6, 'Sleet/Hail/Freezing Rain':7, 'Blowing Sand/Dirt':8,\n",
    "               'Severe Crosswind':9, 'Partly Cloudy':10}\n",
    "df_ac['WEATHER_int'] = df_ac.WEATHER.apply(lambda x: weather_dict.get(x)).fillna(0).astype(int)\n",
    "\n",
    "roadcond_dict = {'Wet':1, 'Dry':2, 'Unknown':3, 'Snow/Slush':4, 'Ice':5, 'Other':6,\n",
    "                'Sand/Mud/Dirt':7, 'Standing Water':8, 'Oil':9}\n",
    "df_ac['ROADCOND_int'] = df_ac.ROADCOND.apply(lambda x: roadcond_dict.get(x)).fillna(0).astype(int)\n",
    "\n",
    "light_dict = {'Daylight':1, 'Dark - Street Lights On':2, 'Dark - No Street Lights':3,\n",
    "             'Unknown':4, 'Dusk':5, 'Dawn':6, 'Dark - Street Lights Off':7,\n",
    "             'Other':8, 'Dark - Unknown Lighting':9}\n",
    "df_ac['LIGHTCOND_int'] = df_ac.LIGHTCOND.apply(lambda x: light_dict.get(x)).fillna(0).astype(int)\n",
    "\n",
    "\n",
    "#View the first 5 rows\n",
    "df_ac[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the data is normalized and ready to be used. We can try to have a little scoop in the attributes that we want to use to compare with the Severity of the accident. For this aspect we will use the raw data, to represent the three main attributes on this case study Weather, Road and Light conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a few graphs to visualize the amount of accidents per year and condition (Weather, Road and Light) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a base for the graphic representation\n",
    "def create_barh_plot(df_ac, title, save, color):\n",
    "    plt.figure(figsize=(15,7))\n",
    "    ax = df_ac.plot(kind='barh', color = color )\n",
    "    plt.xlabel('Counts')\n",
    "    plt.ylabel('Types')\n",
    "    ax.xaxis.label.set_fontsize(18)\n",
    "    ax.yaxis.label.set_fontsize(18)\n",
    "    graph_title = title\n",
    "    plt.title(graph_title, fontsize = 20, fontweight = 'bold')\n",
    "    graph_title_for_save = save\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Severity of accidents representation\n",
    "severity = df_ac.SEVERITYCODE.value_counts()\n",
    "print(severity)\n",
    "create_barh_plot(severity, 'Severity Type', 'SEVERITYCODE', 'indianred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a visualization of the accident severity type, when 2 represents \"Injury\", with more than 57.000 accidents, and 1 is the total with \"Propriety damage\", more than 132.000. Unfortunately, the data is not provided with a most deeply on other types, for example fatalities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accidents per Weather types representation\n",
    "weather = df_ac.WEATHER.value_counts()\n",
    "print(weather)\n",
    "create_barh_plot(weather, 'Weather Type', 'weather_counts', 'lightseagreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally it is expected that the worst types of weather will cause more accidents, but most of the accidents occurred with Clear weather (111.000 accidents). Following it by 33.000 accidents in raining conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accidents per Road Condition types representation\n",
    "roadcond = df_ac.ROADCOND.value_counts()\n",
    "print(roadcond)\n",
    "create_barh_plot(roadcond, 'Road Condition Type', 'roadcond_counts', 'burlywood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we observed the \"Road condition\" we see that mainly the accidents happened with a Dry floor (124.000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accidents per Light Conditions types representation\n",
    "lightcond = df_ac.LIGHTCOND.value_counts()\n",
    "print(lightcond)\n",
    "create_barh_plot(lightcond, 'Light Condition Type', 'lightcond_counts', 'lightcoral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the \"Light conditions\" the largest group represent that the accidents happened by Day.\n",
    "Ending the representation of the three main attributes in study we can see that the accidents happened mostly during the day, with dry roads and with clear sky. Now we will need to process the rest of the information to be able to see the correlation between accident severity and the conditions represented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representation of accidents per year\n",
    "year = df_ac.INCDTTM_year_str.value_counts()\n",
    "print(year)\n",
    "create_barh_plot(year, 'Accidents per Year', 'count', 'darkseagreen')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can see that since 2006 till 2019 (not considering 2020 since the year is not ended) a steady decrease on the number of accidents. Even so, the complete year with less accidents (2019) presents around 9.000 accidents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis:\n",
    "We will start now the analyses of the elements. We will start with saving the attributes that we want to study in a specific dataframe and from there we will create the Machine Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = ['SEVERITYCODE', 'INCDTTM_year_month','WEATHER_int', 'ROADCOND_int', 'LIGHTCOND_int']\n",
    "predict = df_ac[int_cols]\n",
    "predict[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will be putting all the attributes necessary to a numerical code, so that we can be able to process the data and create the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature = predict[['WEATHER_int', 'ROADCOND_int', 'LIGHTCOND_int']]\n",
    "Feature = pd.concat([Feature,pd.get_dummies(predict['SEVERITYCODE'])], axis=1)\n",
    "\n",
    "Feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Feature\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, decided to see the correlation between each attribute. First with Severity code all together, and then the Severity code separated in the two types of accident severity, using the One Hot Encoding technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = predict.corr()\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it seems that Road and Light conditions have the greater correlation with the accident severity variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = Feature.corr()\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprising looks like the same attributes are important for both types of accident severity, Road and Light conditions. Other variable ta have great correlation is in both graphs the Light and Weather condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_corr = predict[['WEATHER_int', 'ROADCOND_int', 'LIGHTCOND_int']].corr()\n",
    "ax = sns.heatmap(predict_corr, cmap='coolwarm', annot=True, fmt='.1f')\n",
    "ax.figure.set_size_inches(15,5)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Features')\n",
    "ax.xaxis.label.set_fontsize(14)\n",
    "ax.yaxis.label.set_fontsize(14)\n",
    "graph_title = 'Pearson R Correlation for Possible Features'\n",
    "plt.title(graph_title, fontsize = 17)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last form of representation, decided to use the Pearson R correlation to represent the correlation between the variables to study face the Severity attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to get the Panda dataframe into a Numpy array, to be able to get the model running.\n",
    "\n",
    "X - feature vector (\"WEATHER_int\", \"ROADCOND_int\" and \"LIGHTCOND_int\")\n",
    "\n",
    "y - predicted variable (\"SEVERITYCODE\")\n",
    "\n",
    "We decided to use three types of models to run, to see the best to provide an answer to our issue: Decision Tree, K Nearest Neighbor (KNN), Logistic Regression and Support Vector Machine (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the aspects to have in consideration regarding the models in use:\n",
    "- Precision quantifies the number of positive class predictions that actually belong to the positive class.\n",
    "- Recall quantifies the number of positive class predictions made out of all positive examples in the dataset.\n",
    "- F-Measure provides a single score that balances both the concerns of precision and recall in one number.\n",
    "- Jaccard Index compares members for two sets to see which members are shared and which are distinct. It’s a measure of similarity for the two sets of data, with a range from 0% to 100%. The higher the percentage, the more similar the two populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = predict[['SEVERITYCODE']].values\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = predict[['WEATHER_int', 'ROADCOND_int', 'LIGHTCOND_int']].values\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to divide the data set into Training and test set. It was decided to attribute 20% of the data set to the Test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, jaccard_similarity_score, f1_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "shift_ml = 'ML for Shift \\n'\n",
    "shift_ml += '\\t X_train: ' + str(X_train.shape[0]) + '\\n'\n",
    "shift_ml += '\\t y_train: ' + str(y_train.shape[0]) + '\\n'\n",
    "shift_ml += '\\t X_test: ' + str(X_test.shape[0]) + '\\n'\n",
    "shift_ml += '\\t y_test: ' + str(y_test.shape[0]) + '\\n'\n",
    "print(shift_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the base for the visual representation of each section\n",
    "def heatmap_for_confusion(df):\n",
    "    fix,ax = plt.subplots(figsize = (15, 7))\n",
    "    sns.heatmap(df, annot = True, fmt = 'd', cmap= 'Greens')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    graph_title = 'Heatmap for Confusion Matrix for Machine Learning Algorithms' \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "print(model)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_DecisionTree = model.predict(X_test)\n",
    "\n",
    "matrix_DecisionTree = confusion_matrix(y_test, y_DecisionTree)\n",
    "print(matrix_DecisionTree)\n",
    "\n",
    "class_report = classification_report(y_test, y_DecisionTree)\n",
    "print(class_report)\n",
    "heatmap_for_confusion(matrix_DecisionTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbor(KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.metrics import accuracy_score\n",
    "ks = range(1, 10)\n",
    "mean_accuracy = []\n",
    "\n",
    "for n in ks:\n",
    "    knn_model  = knn(n_neighbors = n)\n",
    "    knn_model.fit(X_train, np.ravel(y_train, order='C'))\n",
    "    knn_yhat = knn_model.predict(X_test)\n",
    "    mean_accuracy.append(accuracy_score(y_test, knn_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(mean_accuracy)):\n",
    "    print(\"k = {} has a Score = {} \".format(i + 1, mean_accuracy[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(ks, mean_accuracy)\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Testing Accuracy Values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNN(n_neighbors = 4)\n",
    "print(model)\n",
    "\n",
    "model.fit(X_train, np.ravel(y_train, order='C'))\n",
    "y_pred_knn = model.predict(X_test)\n",
    "print(y_pred_knn[:5])\n",
    "\n",
    "matrix = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "class_report = classification_report(y_test, y_pred_knn)\n",
    "print(class_report)\n",
    "heatmap_for_confusion(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "LogisticRegression(solver='lbfgs')\n",
    "\n",
    "LRmodel = LogisticRegression()\n",
    "print(LRmodel)\n",
    "LRmodel.fit(X_train, np.ravel(y_train, order='C'))\n",
    "y_pred_logReg = model.predict(X_test)\n",
    "# print(y_pred_logReg)\n",
    "\n",
    "matrix_logReg = confusion_matrix(y_test, y_pred_logReg)\n",
    "class_report = classification_report(y_test, y_pred_logReg)\n",
    "print(class_report)\n",
    "\n",
    "heatmap_for_confusion(matrix_logReg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "SVM_model = svm.SVC(kernel='rbf')\n",
    "SVM_model.fit(X_train, np.ravel(y_train, order='C')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = SVM_model.predict(X_test)\n",
    "yhat [0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_yhat = SVM_model.predict(X_test)\n",
    "print(\"SVM Jaccard index: %.2f\" % jaccard_similarity_score(y_test, SVM_yhat))\n",
    "print(\"SVM F1-score: %.2f\" % f1_score(y_test, SVM_yhat, average='weighted') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation using Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_output_scores = 'Jaccard and F1 Scores \\n'\n",
    "ml_output_scores += '\\t Decision Trees Scores: ' + '\\n'\n",
    "ml_output_scores += '\\t\\t f1 score: ' +  str(f1_score(y_test, y_DecisionTree, average='weighted'))+ '\\n'\n",
    "ml_output_scores += '\\t\\t jaccard score: ' +  str(jaccard_similarity_score(y_test, y_DecisionTree))+ '\\n'\n",
    "ml_output_scores += '\\n'\n",
    "ml_output_scores += '\\t KNN Scores: ' + '\\n'\n",
    "ml_output_scores += '\\t\\t f1 score: ' +  str(f1_score(y_test, y_pred_knn, average='weighted')) + '\\n'\n",
    "ml_output_scores += '\\t\\t jaccard score: ' +  str(jaccard_similarity_score(y_test, y_pred_knn)) +'\\n'\n",
    "ml_output_scores += '\\n'\n",
    "ml_output_scores += '\\t Logistic Regression Scores: ' + '\\n'\n",
    "ml_output_scores += '\\t\\t f1 score: ' +  str(f1_score(y_test, y_pred_logReg, average='weighted')) + '\\n'\n",
    "ml_output_scores += '\\t\\t jaccard score: ' +  str(jaccard_similarity_score(y_test, y_pred_logReg))+ '\\n'\n",
    "ml_output_scores += '\\n'\n",
    "ml_output_scores += '\\t Support Vector Machine: ' + '\\n'\n",
    "ml_output_scores += '\\t\\t f1 score: ' +  str(f1_score(y_test, yhat, average='weighted')) + '\\n'\n",
    "ml_output_scores += '\\t\\t jaccard score: ' +  str(jaccard_similarity_score(y_test, yhat))+ '\\n'\n",
    "ml_output_scores += '\\n'\n",
    "print(ml_output_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see how the models performed\n",
    "models = [\n",
    "          KNN(),\n",
    "          LogisticRegression(),\n",
    "          DecisionTreeClassifier(),\n",
    "          svm.SVC()\n",
    "]\n",
    "CV=5\n",
    "cv_df = pd.DataFrame(index = range(CV*len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, X, y, scoring = 'accuracy', cv = CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "    cv_df = pd.DataFrame(entries, columns = ['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box-Whisker Plot of the outputs of the models used above.\n",
    "f, ax = plt.subplots(figsize = (15,5))\n",
    "ax = sns.boxplot(x = 'model_name', y = 'accuracy', data = cv_df, palette='rocket_r')\n",
    "ax = sns.stripplot(x = 'model_name', y= 'accuracy', data= cv_df, size = 8, jitter = True, color = 'black',\n",
    "                   edgecolor = 'yellow', linewidth = 2)\n",
    "plt.title ('Box and Whisker Plot of Machine Learning Performance', fontsize = 20, fontweight = 'bold')\n",
    "ax.set_xlabel('Machine Learning Technique', fontsize = 14)\n",
    "ax.set_ylabel('Accuracy', fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
